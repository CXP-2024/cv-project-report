\section{Problem Statement}
Our aim is to create a fast, real-time emotion recognition system for videos on edge devices, tackling the challenges of (1) robust face detection and (2) achieving human-level emotion classification accuracy. Specifically, we aim to achieve:

\begin{itemize}
    \item Processing speed of at least 20 frames per second on laptops with integrated GPUs
    \item Emotion recognition accuracy comparable to trained humans (approximately 60\%)
    \item Robustness across various lighting conditions, face orientations, and video resolutions
\end{itemize}

The solution must be optimized for resource-constrained environments while maintaining acceptable accuracy, making it suitable for deployment edge devices.

\subsection{Readings}
Since our input is full-scene videos rather than isolated portraits, we first need to locate faces before recognizing emotions. We reviewed prior works on face detection like ``ASFD: Automatic and Scalable Face Detector''\@\cite{ASFD} and facial landmark localization, and for emotion classification, we explored efficient CNN architectures such as ``EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,''\@\cite{EfficientNet} prioritizing models with fewer parameters and shallow structures to balance speed and accuracy.
\subsection{Dataset used}
We use the fer2013 dataset, a widely used emotion recognition dataset. It contains 35,887 grayscale facial images (48Ã—48 pixels) labeled with seven emotions: anger, disgust, fear, happiness, sadness, surprise, and neutral. The dataset is collected from diverse sources and divided into training, validation, and test sets to enable rigorous evaluation of our model's generalization capability.

\subsection{Evaluation Methodology}
We employ a multi-faceted evaluation approach:

\begin{enumerate}
    \item \textbf{Algorithmic performance:} We measure both classification accuracy against the test set and computational efficiency (processing time per frame). Our target is to achieve accuracy comparable to trained humans (nearly 60\%) while maintaining real-time processing speeds (more than 20 FPS).
    
    \item \textbf{Real-world testing:} We evaluate our system using real-life video recordings at various resolutions and distances to assess robustness. This includes scenarios with multiple faces, changing lighting conditions, and varying face orientations.
    
    \item \textbf{Optimization evaluation:} We systematically test different optimizations, including resolution reduction, localized detection, and dynamic resolution adjustment, to determine their impact on processing speed and detection accuracy.
    
    \item \textbf{Hardware deployment:} The final evaluation involves deploying our solution on consumer hardware (laptops with integrated GPUs) and potentially smartphones to confirm real-time capabilities on target platforms.
\end{enumerate}

Success is defined as achieving emotion classification accuracy comparable to trained humans while maintaining real-time processing capabilities on standard consumer hardware.
