\section{Problem Statement}
Our goal is to develop a fast and efficient algorithm for emotion recognition in videos that can operate in real-time on consumer hardware. The system must overcome two primary challenges: (1) accurately detecting faces in varied scenarios and (2) correctly classifying emotions with performance comparable to human capability. Specifically, we aim to achieve:

\begin{itemize}
    \item Processing speed of at least 20 frames per second on laptops with integrated GPUs
    \item Emotion recognition accuracy comparable to trained humans (approximately 60\%)
    \item Robustness across various lighting conditions, face orientations, and video resolutions
\end{itemize}

The solution must be optimized for resource-constrained environments while maintaining acceptable accuracy, making it suitable for deployment on standard laptops or even smartphones.

\subsection{Readings}
There are many prior works related to face detection and emotion recognition. Since our input consists of full scene videos rather than isolated portraits, we must first locate faces before analyzing emotions. We reviewed face detection literature such as ``ASFD: Automatic and Scalable Face Detector"\cite{ASFD} and facial landmark localization techniques. For emotion classification, we examined efficient CNN architectures like ``EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"\cite{EfficientNet}, focusing on models with fewer parameters and shallower structures to reduce computational demands while maintaining accuracy.

\subsection{Dataset used}
We use the fer2013 dataset, which is a widely used benchmark for emotion recognition. It contains 35,887 grayscale facial images (48Ã—48 pixels) labeled with seven emotions: anger, disgust, fear, happiness, sadness, surprise, and neutral. The dataset is collected from diverse sources and divided into training, validation, and test sets to enable rigorous evaluation of our model's generalization capability.

\subsection{Evaluation Methodology}
We employ a multi-faceted evaluation approach:

\begin{enumerate}
    \item \textbf{Algorithmic performance:} We measure both classification accuracy against the test set and computational efficiency (processing time per frame). Our target is to achieve accuracy comparable to trained humans (nearly 60\%) while maintaining real-time processing speeds (more than 20 FPS).
    
    \item \textbf{Real-world testing:} We evaluate our system using real-life video recordings at various resolutions and distances to assess robustness. This includes scenarios with multiple faces, changing lighting conditions, and varying face orientations.
    
    \item \textbf{Optimization evaluation:} We systematically test different optimizations, including resolution reduction, localized detection, and dynamic resolution adjustment, to determine their impact on processing speed and detection accuracy.
    
    \item \textbf{Hardware deployment:} The final evaluation involves deploying our solution on consumer hardware (laptops with integrated GPUs) and potentially smartphones to confirm real-time capabilities on target platforms.
\end{enumerate}

Success is defined as achieving emotion classification accuracy comparable to trained humans while maintaining real-time processing capabilities on standard consumer hardware.
