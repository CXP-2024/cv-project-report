\section{Future Improvements}
\subsection{Distorted face detection}
In a video, faces are in different poses, which may cause problems for detection. Actually, in experiments, the model failed to detect the distorted faces. We also consider using a stronger model to fix this problem, however, it may cost a lot. 
So in order to let the model work properly, we need to transform the face back to the right position. We can use SIFT for this function. The eyes and edge of the lips are often keypoints, and as the model for finding the faces actually detect the eyes and lips, we can just select these points as the keypoint and further detect it. (or just the 68 points detected by the model).
Then we can use homography transformation to find the original picture of the distorted faces and then use the model to detect the position of the face. We'll try this in the future.

\subsection{New Datasets and Models}
We are currently considering using new datasets to train our model, such as AffectNet. Some of these datasets contain combined emotions, which present two emotions simultaneously, providing a more realistic representation of human expressions. Additionally, we plan to explore better detection models (e.g., YOLO) to improve both accuracy and efficiency. Further research will focus on identifying additional techniques to enhance the model's overall performance.

\subsection{Real-time Detection on Edge Devices}
We aim to deploy our model on local edge devices, such as mobile phones or Raspberry Pi systems. This implementation would allow for emotion detection using only the device's built-in camera, enabling real-time analysis of people's emotions. This represents a practical and valuable application of our research with numerous potential use cases.
